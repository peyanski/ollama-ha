name: "Ollama"
slug: "ollama-ha"
version: "0.2.0"
description: "Run Ollama LLM server locally on your Home Assistant instance."
url: "https://github.com/peyanski/ollama-ha"
startup: services
init: false
image: ghcr.io/peyanski/{arch}-ollama-ha
arch:
  - amd64
  - aarch64
ports:
  11434/tcp: 11434
host_network: true
panel_icon: mdi:brain
video: true

options:
  gpu: false
schema:
  gpu: bool

environment:
  HOME: "/data"
  OLLAMA_HOST: "0.0.0.0:11434"

map:
  - share:rw


